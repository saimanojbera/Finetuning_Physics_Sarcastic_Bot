{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "x2rBa9T7-fHi",
        "outputId": "c024681d-7872-4f6c-d98d-a2f6ac1eb5d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.21.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib tiktoken openai numpy wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOzPVqs5A1fg",
        "outputId": "f4d3ba63-da27-4e01-a920-6deebeefd0b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n"
      ],
      "metadata": {
        "id": "3ysZG51Ve0Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import tiktoken # for token counting\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "DePBVtDeiVWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "from tiktoken import get_encoding\n",
        "\n",
        "def validate_and_estimate_finetuning_data(file_path):\n",
        "    # Setup\n",
        "    format_errors = defaultdict(int)\n",
        "    token_counts = []\n",
        "    total_tokens = 0\n",
        "    encoding = get_encoding(\"cl100k_base\")  # For OpenAI models\n",
        "\n",
        "\n",
        "    # Load the dataset\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        dataset = [json.loads(line) for line in f]\n",
        "\n",
        "    for idx, ex in enumerate(dataset):\n",
        "        if not isinstance(ex, dict):\n",
        "            format_errors[\"data_type\"] += 1\n",
        "            continue\n",
        "\n",
        "        messages = ex.get(\"messages\", None)\n",
        "        if not messages:\n",
        "            format_errors[\"missing_messages_list\"] += 1\n",
        "            continue\n",
        "\n",
        "        # Validate format\n",
        "        conversation_tokens = 0\n",
        "        assistant_message_found = False\n",
        "\n",
        "        for message in messages:\n",
        "            if \"role\" not in message or \"content\" not in message:\n",
        "                format_errors[\"message_missing_key\"] += 1\n",
        "                continue\n",
        "\n",
        "            if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
        "                format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "            if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
        "                format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "            content = message.get(\"content\", None)\n",
        "            function_call = message.get(\"function_call\", None)\n",
        "\n",
        "            if (not content and not function_call) or not isinstance(content, str):\n",
        "                format_errors[\"missing_content\"] += 1\n",
        "\n",
        "            # Count tokens for each message\n",
        "            try:\n",
        "                message_tokens = len(encoding.encode(message.get(\"content\", \"\")))\n",
        "                conversation_tokens += message_tokens\n",
        "            except Exception as e:\n",
        "                format_errors[\"tokenization_error\"] += 1\n",
        "\n",
        "            if message.get(\"role\") == \"assistant\":\n",
        "                assistant_message_found = True\n",
        "\n",
        "        if not assistant_message_found:\n",
        "            format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "        token_counts.append(conversation_tokens)\n",
        "        total_tokens += conversation_tokens\n",
        "\n",
        "    # Output results\n",
        "    return {\n",
        "        \"format_errors\": dict(format_errors),\n",
        "        \"token_counts\": token_counts,\n",
        "        \"total_tokens\": total_tokens,\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "_2hejk84iafM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_File_Path = \"/content/drive/MyDrive/Finetuned_Physics_Sarcasticbot/physics_train.jsonl\"\n",
        "validation_File_Path = \"/content/drive/MyDrive/Finetuned_Physics_Sarcasticbot/Physics_test.jsonl\""
      ],
      "metadata": {
        "id": "jPk_kTAsil4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Training data\n",
        "result = validate_and_estimate_finetuning_data(training_File_Path)\n",
        "\n",
        "# Print Results\n",
        "print(\"Training Data\")\n",
        "print(\"Format Errors:\", result[\"format_errors\"])\n",
        "print(\"Token Counts per Conversation:\", result[\"token_counts\"])\n",
        "print(\"Total Tokens:\", result[\"total_tokens\"])\n",
        "\n",
        "result = validate_and_estimate_finetuning_data(validation_File_Path)\n",
        "\n",
        "## Test dataset\n",
        "print(\"\\n\\nTest Data\")\n",
        "print(\"Format Errors:\", result[\"format_errors\"])\n",
        "print(\"Token Counts per Conversation:\", result[\"token_counts\"])\n",
        "print(\"Total Tokens:\", result[\"total_tokens\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opYjJ2z8jski",
        "outputId": "5412db5c-e4e4-43f5-87ab-eb36022e8252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data\n",
            "Format Errors: {}\n",
            "Token Counts per Conversation: [64, 62, 65, 65, 61, 82, 61, 60, 67, 71, 63, 57, 71, 65, 71, 57, 71, 68, 63, 61, 64, 64, 64, 72, 61, 60, 64, 68, 64, 61, 66, 59, 69, 71, 63, 67, 70, 60, 67, 58, 58, 60, 67, 72, 61, 57, 65, 57, 61, 65, 65, 70, 58, 65, 65, 58, 68, 64, 65, 72, 76, 60, 62, 68, 60, 59, 65, 70, 67, 58, 64, 62, 65, 56, 68, 62, 54, 65, 70, 61, 71, 70, 57, 62, 61, 71, 53, 62, 55, 64, 66, 58, 69, 68, 66, 65, 56, 60, 71, 63]\n",
            "Total Tokens: 6410\n",
            "\n",
            "\n",
            "Test Data\n",
            "Format Errors: {}\n",
            "Token Counts per Conversation: [65, 72, 62, 76, 68, 63, 73, 59, 72, 70, 64, 72, 70, 78, 59, 67, 74, 67, 66, 65, 64, 70, 75, 56, 72]\n",
            "Total Tokens: 1699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "uhpz59FIj525",
        "outputId": "c07a5e6d-daef-4ebf-cd08-a231722b101e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msaimanojbera\u001b[0m (\u001b[33mberasaimanoj\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## create a client\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "# Function to check if a file already exists on OpenAI\n",
        "def get_existing_file_id(filename):\n",
        "    files = client.files.list()\n",
        "    for file in files.data:\n",
        "        if file.filename == filename:\n",
        "            return file.id  # Return the existing file ID\n",
        "    return None  # File does not exist\n",
        "\n",
        "# Function to delete a file by ID\n",
        "def delete_file(file_id):\n",
        "    response = client.files.delete(file_id)\n",
        "    return response.deleted\n",
        "\n",
        "# Check and delete training file\n",
        "file_name = \"/content/drive/MyDrive/Finetuned_Physics_Sarcasticbot/physics_train.jsonl\"\n",
        "training_file_id = get_existing_file_id(file_name)\n",
        "if training_file_id:\n",
        "    print(f\"Deleting existing training file: {training_File_Path}\")\n",
        "    delete_file(training_file_id)\n",
        "\n",
        "# Check and delete validation file\n",
        "file_name = \"/content/drive/MyDrive/Finetuned_Physics_Sarcasticbot/Physics_test.jsonl\"\n",
        "validation_file_id = get_existing_file_id(file_name)\n",
        "if validation_file_id:\n",
        "    print(f\"Deleting existing validation file: {validation_File_Path}\")\n",
        "    delete_file(validation_file_id)\n",
        "\n",
        "# Upload the training file\n",
        "training = client.files.create(\n",
        "    file=open(training_File_Path, \"rb\"),\n",
        "    purpose=\"fine-tune\"\n",
        ")\n",
        "print(f\"Training file uploaded: {training.id}\")\n",
        "\n",
        "# Upload the validation file\n",
        "validation = client.files.create(\n",
        "    file=open(validation_File_Path, \"rb\"),\n",
        "    purpose=\"fine-tune\"\n",
        ")\n",
        "print(f\"Validation file uploaded: {validation.id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeJfkEnbmISO",
        "outputId": "34f6a691-adff-48ef-ccd0-4d5da5e04a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training file uploaded: file-Y95HvxsPAzdS96xyJgxh2c\n",
            "Validation file uploaded: file-AsYMPuvcgDtor4cMrsX9Bn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## List all the files to choose its id for fine tuning with it's data\n",
        "files = client.files.list()\n",
        "print(files.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIBpu6ypm1d-",
        "outputId": "807bef75-7d00-4dac-908a-b33ce5d8debe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FileObject(id='file-AsYMPuvcgDtor4cMrsX9Bn', bytes=11904, created_at=1739675699, filename='Physics_test.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-Y95HvxsPAzdS96xyJgxh2c', bytes=44045, created_at=1739675699, filename='physics_train.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-SNZ5ivgeGU6PHea6YVae99', bytes=884, created_at=1739517027, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None), FileObject(id='file-FLN6xi2T536n5U8t28s5zM', bytes=9659392, created_at=1739515980, filename='val.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-FLVeKPvFraYfFMJBFX9Lr6', bytes=18887348, created_at=1739515979, filename='train.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-9pX8RLawjekzKDVoVPsDmP', bytes=9651592, created_at=1739515290, filename='val.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-AoXJvECKCgVGxXZ5DWVdcP', bytes=18871748, created_at=1739515289, filename='train.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-TzqetwFH77rSakcaK2RsgC', bytes=9650392, created_at=1739515017, filename='val.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-3GWHtb1n5HJRNvgpgRCZn5', bytes=18869348, created_at=1739515016, filename='train.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-LS3ZhJ2DpvSt4tvgzRdP14', bytes=9633292, created_at=1739512359, filename='val.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-HN8K834NkbTqEVPFBXZGhv', bytes=18628408, created_at=1739512355, filename='train_filtered.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-XGHGGABqAc2zSJ1veVXTSN', bytes=9633292, created_at=1739512026, filename='val.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-LspHFbqEvUzvUzj2Amtrtv', bytes=18835148, created_at=1739512024, filename='train.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-JK1wdBDiVpGjVbaXCwke1W', bytes=9633292, created_at=1739505504, filename='val.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None), FileObject(id='file-Jx29Qms9yoJDvWNjzoPfp8', bytes=18835148, created_at=1739505499, filename='train.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Paste the file id into the training_file parameter and choose the model and adjust the hyperparameters if you want to tune it\n",
        "job = client.fine_tuning.jobs.create(\n",
        "    training_file= training.id,\n",
        "    validation_file=validation.id,\n",
        "    model = \"gpt-3.5-turbo-0125\",\n",
        "    method={\n",
        "        \"type\": \"supervised\",\n",
        "        \"supervised\": {\n",
        "            \"hyperparameters\": {\n",
        "                \"n_epochs\": 5,  # Number of epochs\n",
        "                \"batch_size\": 8,  # Batch size\n",
        "                \"learning_rate_multiplier\": 0.7,  # Learning rate scaling factor\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    integrations= [\n",
        "        {\n",
        "            \"type\": \"wandb\",\n",
        "            \"wandb\": {\n",
        "                \"project\": \"Finetuned_Physics_Sacastic_bot\",\n",
        "                \"tags\": [\"Physics\", \"Sarcastic\", \"finetuning\"]\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "print(job)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdnIek-JnJe8",
        "outputId": "10971065-5b02-4c23-eb5e-78f36a5228c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FineTuningJob(id='ftjob-GzVu6JtwwW2SEONO9vhnDa2j', created_at=1739676182, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=8, learning_rate_multiplier=0.7, n_epochs=5), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-QL5uq9QCQwO0sZxVglpgmpWc', result_files=[], seed=339231874, status='validating_files', trained_tokens=None, training_file='file-Y95HvxsPAzdS96xyJgxh2c', validation_file='file-AsYMPuvcgDtor4cMrsX9Bn', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='Finetuned_Physics_Sacastic_bot', entity=None, name=None, tags=None, run_id='ftjob-GzVu6JtwwW2SEONO9vhnDa2j'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=8, learning_rate_multiplier=0.7, n_epochs=5)), type='supervised'), user_provided_suffix=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Listing all the recent jobs\n",
        "all_jobs = client.fine_tuning.jobs.list(limit=10).data\n",
        "print(all_jobs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6kKvDbxoX-l",
        "outputId": "b77eb2c3-c827-49d0-e135-28bd3c81ad27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FineTuningJob(id='ftjob-GzVu6JtwwW2SEONO9vhnDa2j', created_at=1739676182, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=8, learning_rate_multiplier=0.7, n_epochs=5), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-QL5uq9QCQwO0sZxVglpgmpWc', result_files=[], seed=339231874, status='running', trained_tokens=None, training_file='file-Y95HvxsPAzdS96xyJgxh2c', validation_file='file-AsYMPuvcgDtor4cMrsX9Bn', estimated_finish=1739676674, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='Finetuned_Physics_Sacastic_bot', entity=None, name=None, tags=None, run_id='ftjob-GzVu6JtwwW2SEONO9vhnDa2j'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=8, learning_rate_multiplier=0.7, n_epochs=5)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-gvHtlw7Vrc1AyI9vKfOaLUMH', created_at=1739675795, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=150, learning_rate_multiplier=0.7, n_epochs=5), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-QL5uq9QCQwO0sZxVglpgmpWc', result_files=[], seed=1118221128, status='cancelled', trained_tokens=None, training_file='file-Y95HvxsPAzdS96xyJgxh2c', validation_file='file-AsYMPuvcgDtor4cMrsX9Bn', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='Viceversa_cat_Dog_Gpt4o_fine_tuning', entity=None, name=None, tags=None, run_id='ftjob-gvHtlw7Vrc1AyI9vKfOaLUMH'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=150, learning_rate_multiplier=0.7, n_epochs=5)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-wbfeyaYn7hLFF3sq3DPGuCv6', created_at=1739515987, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-2024-08-06:personal::B0k89vGT', finished_at=1739517023, hyperparameters=Hyperparameters(batch_size=150, learning_rate_multiplier=0.7, n_epochs=5), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-QL5uq9QCQwO0sZxVglpgmpWc', result_files=['file-SNZ5ivgeGU6PHea6YVae99'], seed=444476460, status='succeeded', trained_tokens=259800, training_file='file-FLVeKPvFraYfFMJBFX9Lr6', validation_file='file-FLN6xi2T536n5U8t28s5zM', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='Viceversa_cat_Dog_Gpt4o_fine_tuning', entity=None, name=None, tags=None, run_id='ftjob-wbfeyaYn7hLFF3sq3DPGuCv6'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=150, learning_rate_multiplier=0.7, n_epochs=5)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-EjC2p8oraOT8OWMxiQa071Ji', created_at=1739515297, error=Error(code='invalid_training_file', message=\"The job failed due to an invalid training file. Invalid file format. Line 1, message 3: Input tag 'image_b64' found using 'type' does not match any of the expected tags: 'image_url', 'text'\", param='training_file'), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=150, learning_rate_multiplier=0.7, n_epochs=5), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-QL5uq9QCQwO0sZxVglpgmpWc', result_files=[], seed=108514783, status='failed', trained_tokens=None, training_file='file-AoXJvECKCgVGxXZ5DWVdcP', validation_file='file-9pX8RLawjekzKDVoVPsDmP', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='Viceversa_cat_Dog_Gpt4o_fine_tuning', entity=None, name=None, tags=None, run_id='ftjob-EjC2p8oraOT8OWMxiQa071Ji'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=150, learning_rate_multiplier=0.7, n_epochs=5)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-N2FujSWCAjwxzvks2N0gbdla', created_at=1739515041, error=Error(code='invalid_training_file', message=\"The job failed due to an invalid training file. Invalid file format. Line 1, message 3: Input tag 'image' found using 'type' does not match any of the expected tags: 'image_url', 'text'\", param='training_file'), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=150, learning_rate_multiplier=0.7, n_epochs=5), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-QL5uq9QCQwO0sZxVglpgmpWc', result_files=[], seed=1431976119, status='failed', trained_tokens=None, training_file='file-3GWHtb1n5HJRNvgpgRCZn5', validation_file='file-TzqetwFH77rSakcaK2RsgC', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='Viceversa_cat_Dog_Gpt4o_fine_tuning', entity=None, name=None, tags=None, run_id='ftjob-N2FujSWCAjwxzvks2N0gbdla'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=150, learning_rate_multiplier=0.7, n_epochs=5)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-gRWJLARZh4yYcQRbwyg2FhqN', created_at=1739512477, error=Error(code='exceeded_quota', message='Creating this fine-tuning job would exceed your hard limit, please check your plan and billing details.                     Cost of job ftjob-gRWJLARZh4yYcQRbwyg2FhqN: USD 1565.61. Quota remaining for your project proj_d7TFVNob3wmTZGC32R0cEuZc: USD 118.17.', param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=150, learning_rate_multiplier=0.7, n_epochs=5), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-QL5uq9QCQwO0sZxVglpgmpWc', result_files=[], seed=762652733, status='failed', trained_tokens=None, training_file='file-HN8K834NkbTqEVPFBXZGhv', validation_file='file-LS3ZhJ2DpvSt4tvgzRdP14', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='Viceversa_cat_Dog_Gpt4o_fine_tuning', entity=None, name=None, tags=None, run_id='ftjob-gRWJLARZh4yYcQRbwyg2FhqN'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=150, learning_rate_multiplier=0.7, n_epochs=5)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-Q8sd2zAA4oklaTi7PSGqVO7D', created_at=1739512082, error=Error(code='invalid_training_file', message=\"The job failed due to an invalid training file. Invalid file format. Example 81 No completion or assistant tokens were found in the dataset (possibly because of truncation). It's likely that all assistant tokens are outside of the context window (65536 tokens). Please check your dataset or use a model with a larger context window.\", param='training_file'), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=150, learning_rate_multiplier=0.7, n_epochs=5), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-QL5uq9QCQwO0sZxVglpgmpWc', result_files=[], seed=883124259, status='failed', trained_tokens=None, training_file='file-LspHFbqEvUzvUzj2Amtrtv', validation_file='file-XGHGGABqAc2zSJ1veVXTSN', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='Viceversa_cat_Dog_Gpt4o_fine_tuning', entity=None, name=None, tags=None, run_id='ftjob-Q8sd2zAA4oklaTi7PSGqVO7D'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=150, learning_rate_multiplier=0.7, n_epochs=5)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-TSqqkkLy9Ekw8wS8lnPtgI8S', created_at=1739505988, error=Error(code='invalid_training_file', message=\"The job failed due to an invalid training file. Invalid file format. Example 81 No completion or assistant tokens were found in the dataset (possibly because of truncation). It's likely that all assistant tokens are outside of the context window (65536 tokens). Please check your dataset or use a model with a larger context window.\", param='training_file'), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=150, learning_rate_multiplier=0.7, n_epochs=5), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-QL5uq9QCQwO0sZxVglpgmpWc', result_files=[], seed=1644287040, status='failed', trained_tokens=None, training_file='file-Jx29Qms9yoJDvWNjzoPfp8', validation_file='file-JK1wdBDiVpGjVbaXCwke1W', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='Viceversa_cat_Dog_Gpt4o_fine_tuning', entity=None, name=None, tags=None, run_id='ftjob-TSqqkkLy9Ekw8wS8lnPtgI8S'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=150, learning_rate_multiplier=0.7, n_epochs=5)), type='supervised'), user_provided_suffix=None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Prinint the recent job to get the fine-tuned model name\n",
        "print(all_jobs[0])\n",
        "print(client.fine_tuning.jobs.retrieve(all_jobs[0].id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUUEfNUkpDBq",
        "outputId": "4e83aa70-4be6-46cc-95d7-c94738c15155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FineTuningJob(id='ftjob-GzVu6JtwwW2SEONO9vhnDa2j', created_at=1739676182, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=8, learning_rate_multiplier=0.7, n_epochs=5), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-QL5uq9QCQwO0sZxVglpgmpWc', result_files=[], seed=339231874, status='running', trained_tokens=None, training_file='file-Y95HvxsPAzdS96xyJgxh2c', validation_file='file-AsYMPuvcgDtor4cMrsX9Bn', estimated_finish=1739676674, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='Finetuned_Physics_Sacastic_bot', entity=None, name=None, tags=None, run_id='ftjob-GzVu6JtwwW2SEONO9vhnDa2j'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=8, learning_rate_multiplier=0.7, n_epochs=5)), type='supervised'), user_provided_suffix=None)\n",
            "FineTuningJob(id='ftjob-GzVu6JtwwW2SEONO9vhnDa2j', created_at=1739676182, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=8, learning_rate_multiplier=0.7, n_epochs=5), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-QL5uq9QCQwO0sZxVglpgmpWc', result_files=[], seed=339231874, status='running', trained_tokens=None, training_file='file-Y95HvxsPAzdS96xyJgxh2c', validation_file='file-AsYMPuvcgDtor4cMrsX9Bn', estimated_finish=1739676677, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='Finetuned_Physics_Sacastic_bot', entity=None, name=None, tags=None, run_id='ftjob-GzVu6JtwwW2SEONO9vhnDa2j'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=8, learning_rate_multiplier=0.7, n_epochs=5)), type='supervised'), user_provided_suffix=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import requests\n",
        "checkpoints = None\n",
        "\n",
        "# Function to get the latest accuracy and loss from checkpoints\n",
        "def get_latest_accuracy(job_id, api_key):\n",
        "    url = f\"https://api.openai.com/v1/fine_tuning/jobs/{job_id}/checkpoints\"\n",
        "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "    checkpoints = response.json().get(\"data\", [])\n",
        "\n",
        "    if not checkpoints:\n",
        "        return None, None  # Return None if no checkpoints are available\n",
        "\n",
        "    # Find the latest checkpoint based on step_number\n",
        "    latest_checkpoint = max(checkpoints, key=lambda c: c[\"step_number\"])\n",
        "    latest_accuracy = latest_checkpoint[\"metrics\"][\"full_valid_mean_token_accuracy\"]\n",
        "    latest_loss = latest_checkpoint[\"metrics\"][\"full_valid_loss\"]\n",
        "    return latest_accuracy, latest_loss\n",
        "\n",
        "# Function to monitor fine-tuning job and print training/validation metrics\n",
        "def monitor_finetuning_progress(job_id, api_key, check_interval=10):\n",
        "    while True:\n",
        "        try:\n",
        "            # Retrieve the fine-tuning job status\n",
        "            job_status = client.fine_tuning.jobs.retrieve(job_id)\n",
        "\n",
        "            # Print basic job details\n",
        "            print(f\"Job ID: {job_status.id}\")\n",
        "            print(f\"Status: {job_status.status}\")\n",
        "\n",
        "            # Check if the job has completed\n",
        "            if job_status.status in [\"succeeded\", \"failed\"]:\n",
        "                print(f\"Fine-tuning job {job_status.status}.\")\n",
        "                model_id = job_status.fine_tuned_model\n",
        "                result_file_id = job_status.result_files[0]\n",
        "                return job_status, model_id, result_file_id\n",
        "\n",
        "            # Retrieve and print the latest accuracy and loss\n",
        "            latest_accuracy, latest_loss = get_latest_accuracy(job_id, api_key)\n",
        "            if latest_accuracy is not None and latest_loss is not None:\n",
        "                print(f\"Latest Accuracy: {latest_accuracy:.3f}\")\n",
        "                print(f\"Latest Loss: {latest_loss:.3f}\")\n",
        "            else:\n",
        "                print(\"No checkpoints available yet.\")\n",
        "\n",
        "            # Wait before the next check\n",
        "            print(f\"Checking again in {check_interval} seconds...\\n\")\n",
        "            time.sleep(check_interval)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}. Retrying in {check_interval} seconds...\\n\")\n",
        "            time.sleep(check_interval)\n",
        "\n",
        "\n",
        "# Replace `fine_tuning_job_id` with your actual job ID\n",
        "fine_tuning_job_id = all_jobs[0].id\n",
        "status, model_name, result_file_id = monitor_finetuning_progress(fine_tuning_job_id, api_key, 10)\n",
        "print(f\"Status: {status}\")\n",
        "print(f\"Model Name: {model_name}\")\n",
        "print(f\"Result file id: {result_file_id}\")"
      ],
      "metadata": {
        "id": "sDZvSiNLpGsw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1016eff-e075-4798-b5bd-0685afeffc14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job ID: ftjob-GzVu6JtwwW2SEONO9vhnDa2j\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-GzVu6JtwwW2SEONO9vhnDa2j\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-GzVu6JtwwW2SEONO9vhnDa2j\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-GzVu6JtwwW2SEONO9vhnDa2j\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-GzVu6JtwwW2SEONO9vhnDa2j\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-GzVu6JtwwW2SEONO9vhnDa2j\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-GzVu6JtwwW2SEONO9vhnDa2j\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-GzVu6JtwwW2SEONO9vhnDa2j\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-GzVu6JtwwW2SEONO9vhnDa2j\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-GzVu6JtwwW2SEONO9vhnDa2j\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-GzVu6JtwwW2SEONO9vhnDa2j\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-GzVu6JtwwW2SEONO9vhnDa2j\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-GzVu6JtwwW2SEONO9vhnDa2j\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-GzVu6JtwwW2SEONO9vhnDa2j\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-GzVu6JtwwW2SEONO9vhnDa2j\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-GzVu6JtwwW2SEONO9vhnDa2j\n",
            "Status: running\n",
            "No checkpoints available yet.\n",
            "Checking again in 10 seconds...\n",
            "\n",
            "Job ID: ftjob-GzVu6JtwwW2SEONO9vhnDa2j\n",
            "Status: succeeded\n",
            "Fine-tuning job succeeded.\n",
            "Status: FineTuningJob(id='ftjob-GzVu6JtwwW2SEONO9vhnDa2j', created_at=1739676182, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-3.5-turbo-0125:personal::B1PfZKHk', finished_at=1739676699, hyperparameters=Hyperparameters(batch_size=8, learning_rate_multiplier=0.7, n_epochs=5), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-QL5uq9QCQwO0sZxVglpgmpWc', result_files=['file-9Tt5HpSKWAmBeyv5yGw5Hi'], seed=339231874, status='succeeded', trained_tokens=38550, training_file='file-Y95HvxsPAzdS96xyJgxh2c', validation_file='file-AsYMPuvcgDtor4cMrsX9Bn', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='Finetuned_Physics_Sacastic_bot', entity=None, name=None, tags=None, run_id='ftjob-GzVu6JtwwW2SEONO9vhnDa2j'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=8, learning_rate_multiplier=0.7, n_epochs=5)), type='supervised'), user_provided_suffix=None)\n",
            "Model Name: ft:gpt-3.5-turbo-0125:personal::B1PfZKHk\n",
            "Result file id: file-9Tt5HpSKWAmBeyv5yGw5Hi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_NpWtv5TOSKA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}